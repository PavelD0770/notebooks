{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nfs/rzamora/miniconda3/envs/cudf_dev_sort/lib/python3.7/site-packages/numba/cuda/envvars.py:16: NumbaDeprecationWarning: \n",
      "Environment variables with the 'NUMBAPRO' prefix are deprecated, found use of NUMBAPRO_NVVM=/usr/local/cuda-9.2/nvvm/lib64/libnvvm.so.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-numbapro-environment-variables\n",
      "  warnings.warn(errors.NumbaDeprecationWarning(msg))\n",
      "/home/nfs/rzamora/miniconda3/envs/cudf_dev_sort/lib/python3.7/site-packages/numba/cuda/envvars.py:16: NumbaDeprecationWarning: \n",
      "Environment variables with the 'NUMBAPRO' prefix are deprecated, found use of NUMBAPRO_LIBDEVICE=/usr/local/cuda-9.2/nvvm/libdevice.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-numbapro-environment-variables\n",
      "  warnings.warn(errors.NumbaDeprecationWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "import dask.dataframe as dd\n",
    "import dask_cudf\n",
    "import cudf\n",
    "import numpy as np\n",
    "\n",
    "import shutil\n",
    "import time\n",
    "import os\n",
    "\n",
    "path = \"dummy_dataset.parquet\"\n",
    "path_shuffled = \"dummy_dataset_shuffled.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nfs/rzamora/miniconda3/envs/cudf_dev_sort/lib/python3.7/site-packages/distributed/dashboard/core.py:74: UserWarning: \n",
      "Port 8787 is already in use. \n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the diagnostics dashboard on a random port instead.\n",
      "  warnings.warn(\"\\n\" + msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:35542\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:39074/status' target='_blank'>http://127.0.0.1:39074/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>8</li>\n",
       "  <li><b>Cores: </b>8</li>\n",
       "  <li><b>Memory: </b>540.95 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://127.0.0.1:35542' processes=8 cores=8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster = LocalCUDACluster()\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_read_meta_cudf 0.6961123943328857\n"
     ]
    }
   ],
   "source": [
    "# Read back the parquet dataset on GPUs `read_parquet`\n",
    "\n",
    "ts = time.time()\n",
    "gddf_read = dask_cudf.read_parquet(path, index=\"timestamp\", strings_to_categorical=True, gather_statistics=True)\n",
    "time_read_meta_cudf = time.time() - ts\n",
    "print(\"time_read_meta_cudf\", time_read_meta_cudf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('1900-12-31 00:00:00')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gddf_read.divisions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gddf_read.npartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for DL\n",
    "\n",
    "cat_names = ['name']\n",
    "cont_names = ['value', 'id1000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step #1 - Catigorify\n",
    "# Note that this is currently accomplished by\n",
    "# `strings_to_categorical` argument to `dask_cudf.read_parquet`\n",
    "\n",
    "# Note that another alternative is to reduce the unique strings for each column\n",
    "# and then use nvcategory.from_strings(my_nvstrings).set_keys(my_keys) to\n",
    "# convert each column with a map_partitions call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_fillna 13.102049350738525\n"
     ]
    }
   ],
   "source": [
    "# Step #2 - Fill NA/NaN\n",
    "ts = time.time()\n",
    "for col in cont_names:\n",
    "    median = gddf_read[col].quantile(0.5).compute()\n",
    "    if gddf_read[col].dtype in ('int64','int32'):\n",
    "        median = int(median)\n",
    "    gddf_read[col] = gddf_read[col].fillna(median)\n",
    "time_fillna = time.time() - ts\n",
    "print(\"time_fillna\", time_fillna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_normalize 26.21594476699829\n"
     ]
    }
   ],
   "source": [
    "# Step #3 - Normalize\n",
    "ts = time.time()\n",
    "gdf_cont = gddf_read[cont_names]\n",
    "means = gdf_cont.mean().compute()\n",
    "stds = gdf_cont.std().compute()\n",
    "for i, name in enumerate(cont_names):\n",
    "    gddf_read[name] = (gddf_read[name]-means[i])/(1e-7+stds[i])\n",
    "    gddf_read[name] = gddf_read[name].astype('float32')\n",
    "time_normalize = time.time() - ts\n",
    "print(\"time_normalize\", time_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id1000         name        value\n",
      "1900-12-31T00:00:00.000   -1.4230555    240152049  0.095782526\n",
      "1900-12-31T00:00:01.000  -0.44275093   1785946901   -1.1133134\n",
      "1900-12-31T00:00:02.000    -1.612792   1413041722  -0.56240535\n",
      "1900-12-31T00:00:03.000   -1.1700737  -1573697422   0.55096644\n",
      "1900-12-31T00:00:04.000  -0.88546914   1070281201   -1.4357302\n"
     ]
    }
   ],
   "source": [
    "print(gddf_read.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets add a new column to shuffle the dataset\n",
    "\n",
    "def _assign_rand(df):\n",
    "    return df.assign(sort_ind=np.random.permutation(len(df)))\n",
    "gddf_read_new = gddf_read.map_partitions(_assign_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_set_index_sort_ind 44.708784341812134\n"
     ]
    }
   ],
   "source": [
    "# Set index to column `sort_ind` to shuffle the dataset\n",
    "\n",
    "ts = time.time()\n",
    "#gddf_read_shuffled = gddf_read_new.set_index('sort_ind')\n",
    "gddf_read_shuffled = dd.shuffle.set_index(gddf_read_new, 'sort_ind')\n",
    "time_set_index_sort_ind = time.time() - ts\n",
    "print(\"time_set_index_sort_ind\", time_set_index_sort_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gddf_read_shuffled.divisions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gddf_read_shuffled.npartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show time for shuffle (Shouldn't actually do this step in practice)\n",
    "#\n",
    "# ts = time.time()\n",
    "# gddf_read_shuffled.compute()\n",
    "# time_shuffle = time.time() - ts\n",
    "# print(\"time_shuffle:\",time_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_clean: 2.4299046993255615\n"
     ]
    }
   ],
   "source": [
    "# Write out \"shuffled\" dataset\n",
    "# Total time is shuffle + write (write is currently SLOW here)\n",
    "\n",
    "ts = time.time()\n",
    "if os.path.isdir(path_shuffled):\n",
    "    shutil.rmtree(path_shuffled)\n",
    "time_clean = time.time() - ts\n",
    "print(\"time_clean:\",time_clean)\n",
    "\n",
    "ts = time.time()\n",
    "gddf_read_shuffled.to_parquet(path_shuffled, write_index=False, engine=\"pyarrow\")\n",
    "time_gen_and_write = time.time() - ts\n",
    "print(\"time_gen_and_write:\",time_gen_and_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_read_meta 0.07684016227722168\n"
     ]
    }
   ],
   "source": [
    "# Read back the \"shuffled\" parquet dataset\n",
    "ts = time.time()\n",
    "gddf_read_shuffled_2 = dask_cudf.read_parquet(path_shuffled, index=False, gather_statistics=True)\n",
    "time_read_meta = time.time() - ts\n",
    "print(\"time_read_meta\", time_read_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id1000</th>\n",
       "      <th>name</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36649</th>\n",
       "      <td>-0.601083</td>\n",
       "      <td>-997114637</td>\n",
       "      <td>-0.167866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36650</th>\n",
       "      <td>0.568863</td>\n",
       "      <td>240152049</td>\n",
       "      <td>0.685256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36651</th>\n",
       "      <td>-1.771028</td>\n",
       "      <td>793428781</td>\n",
       "      <td>-1.668564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36652</th>\n",
       "      <td>-0.316501</td>\n",
       "      <td>1159665297</td>\n",
       "      <td>0.322561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36653</th>\n",
       "      <td>-0.790803</td>\n",
       "      <td>1656922212</td>\n",
       "      <td>-1.700507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id1000        name     value\n",
       "36649 -0.601083  -997114637 -0.167866\n",
       "36650  0.568863   240152049  0.685256\n",
       "36651 -1.771028   793428781 -1.668564\n",
       "36652 -0.316501  1159665297  0.322561\n",
       "36653 -0.790803  1656922212 -1.700507"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gddf_read_shuffled_2.compute().tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
