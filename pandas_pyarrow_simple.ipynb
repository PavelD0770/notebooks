{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import fastparquet\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import random\n",
    "import string\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Define input parameters\n",
    "tmpdir = '.'                    # Parent Directory to write/read data\n",
    "size   = 1000000                # Length of global dataframe\n",
    "npart  = 8                      # Number of dataframe partitions\n",
    "nprocs = 8                      # Number of processes to use\n",
    "path = tmpdir + '/test_dataset' # Parquet dataset directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating parallel I/O with Pandas, Apache Arrow and Parquet\n",
    "\n",
    "- Rick Zamora (5/14/2019)\n",
    "\n",
    "\n",
    "This notebook includes a simple exploration of the pyarrow-parquet writer/reader interface. The general goal is to discover the requirements for efficient parallel processing of partitioned pandas datasets.\n",
    "\n",
    "\n",
    "To begin this exploration, we generate a *large* global pandas dataframe, and partition it into `npart` pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>875000</th>\n",
       "      <td>C</td>\n",
       "      <td>875000</td>\n",
       "      <td>0.313344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875001</th>\n",
       "      <td>A</td>\n",
       "      <td>875001</td>\n",
       "      <td>0.300352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875002</th>\n",
       "      <td>D</td>\n",
       "      <td>875002</td>\n",
       "      <td>0.214996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875003</th>\n",
       "      <td>D</td>\n",
       "      <td>875003</td>\n",
       "      <td>0.208321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875004</th>\n",
       "      <td>A</td>\n",
       "      <td>875004</td>\n",
       "      <td>0.447630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        a       b         c\n",
       "875000  C  875000  0.313344\n",
       "875001  A  875001  0.300352\n",
       "875002  D  875002  0.214996\n",
       "875003  D  875003  0.208321\n",
       "875004  A  875004  0.447630"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the global dataframe\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "    'a': [random.choice(['A', 'B', 'C', 'D']) for c in range(size)], \n",
    "    'b': np.arange(size),\n",
    "    'c': [random.random() for c in range(size)], \n",
    "    }\n",
    ")\n",
    "\n",
    "# Break the global dataframe into partitions\n",
    "lsize = size//npart\n",
    "df_part = [ df.iloc[ x * (size//npart) : size if (x == npart-1) else (x * (size//npart) + size//npart) ] for x in range(npart) ]\n",
    "df_part[npart-1].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we are showing the `head()` of the last partition. In order to write each of these partions, we define the `write_partition` function, which returns a *metadata* object for each partition. Note that the metadata object is actually a dictionary, with `'meta'`, and `'schema'` keys. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define `write_partition` funciton\n",
    "def write_partition(df, filename):\n",
    "    t = pa.Table.from_pandas(df)\n",
    "    metadata_list = []\n",
    "    with open(filename, \"wb\") as fil:\n",
    "        pq.write_table(\n",
    "            t, \n",
    "            fil, \n",
    "            metadata_collector=metadata_list\n",
    "        )\n",
    "    # Return metadata & schema\n",
    "    return {'meta': metadata_list[0], 'schema': t.schema}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to write the pandas dataframe in parallel, we can use a simple `multiprocessing.Pool` approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40 ms, sys: 72 ms, total: 112 ms\n",
      "Wall time: 194 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['part.2.parquet',\n",
       " 'part.6.parquet',\n",
       " 'part.7.parquet',\n",
       " 'part.4.parquet',\n",
       " 'part.5.parquet',\n",
       " 'part.3.parquet',\n",
       " 'part.0.parquet',\n",
       " 'part.1.parquet']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write the datafram in parallel (to multiple parquet files)\n",
    "def fwrite(args):\n",
    "    i = args[0]\n",
    "    part = args[1]\n",
    "    ipath = args[2]\n",
    "    filename = ipath + \"/part.%i.parquet\" % i\n",
    "    meta = write_partition(part, filename)\n",
    "    return meta\n",
    "\n",
    "fargs = [ [i, part, path] for i, part in enumerate(df_part) ]\n",
    "os.system('rm -rf ' + path)\n",
    "os.system('mkdir ' + path)\n",
    "p = Pool(nprocs)\n",
    "%time meta = p.map(fwrite, fargs)\n",
    "\n",
    "# Check that there are multiple files\n",
    "files = os.listdir(path)\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that each of the `npart` partitions is written as a separate file called `'part.<part>.parquet'` (where `part` is just an integer in this case). Since we are collecting the metadata/schema information for each of the partitions that is written, we can also write the schema information to a file called `_common_metadata` uisng the `write_metadata()` function available in `pyarrow.parquet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd_meta = None\n",
    "#for md in meta:\n",
    "#    pd_meta_part = json.loads(md['schema'].metadata[b\"pandas\"].decode(\"utf8\"))\n",
    "#    if not pd_meta:\n",
    "#        pd_meta = pd_meta_part\n",
    "#    elif pd_meta['index_columns'] and pd_meta_part['index_columns']:\n",
    "#        pd_meta['index_columns'][0]['stop'] = max( pd_meta_part['index_columns'][0]['stop'], pd_meta['index_columns'][0]['stop'])      \n",
    "#print(pd_meta)\n",
    "\n",
    "# Write a `_common_metadata` (schema) file using the 0th partition\n",
    "# (Note that this is not actually needed...)\n",
    "schema = meta[0]['schema']\n",
    "metadata_path = path + '/_common_metadata' \n",
    "with open(metadata_path, \"wb\") as fil:\n",
    "    pq.write_metadata(schema, fil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the `pq.write_metadata()` call above uses the `schema` object for the 0th partition. This actually means that not **all** information in `_common_metadata` will be true for the entire dataset. More specifically, the `\"index_columns\"` item will have something like this:\n",
    "\n",
    "```[{\"kind\": \"range\", \"name\": null, \"start\": 0, \"stop\": 50000, \"step\": 1}]```\n",
    "            \n",
    "Therefore, the `\"start\"` and `\"stop\"` values will correspond to the 0th partition, rather than the global dataset.\n",
    "\n",
    "Although we just took the time to write out a `_common_metadata` file, it does not currently make much sense to use the file to read back the dataset.  I say this, because that file does not seem include the type of metadata we really need for typical data-processing tasks (such as the number of partitions, the schema, and the row statistics).\n",
    "\n",
    "Assuming that a single-file metadata solution is currently missing, we can temporarily rely on the `ParquetDataset` class in `pyarrow.parquet`. When passed a list of files (or a directory in our case), `ParquetDataset` allows you to iterate through the various `pieces` (partitions) of the global dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 6.49 ms\n"
     ]
    }
   ],
   "source": [
    "%time dataset = pq.ParquetDataset(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 716 Âµs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyarrow._parquet.FileMetaData object at 0x7fcbc9868ae8>\n",
       "  created_by: parquet-cpp version 1.5.1-SNAPSHOT\n",
       "  num_columns: 3\n",
       "  num_rows: 125000\n",
       "  num_row_groups: 1\n",
       "  format_version: 1.0\n",
       "  serialized_size: 991"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pieces = dataset.pieces\n",
    "piece = pieces[0]\n",
    "%time md = piece.get_metadata(lambda fn: pq.ParquetFile(open(fn, mode=\"rb\")))\n",
    "columns = md.schema.names\n",
    "md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, rather than reading the metadata from a dedicated metadata file, we can use the `dataset` obect to read the metadata from each piece (partition).  If we are interested in the row statistics of each partition, we can iterate trhough each file and generate our own `stats` structure. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num-rows': 125000,\n",
       " 'columns': [{'name': 'a', 'min': b'A', 'max': b'D', 'null_count': 0},\n",
       "  {'name': 'b', 'min': 0, 'max': 124999, 'null_count': 0},\n",
       "  {'name': 'c', 'min': 6.62184e-06, 'max': 0.999997, 'null_count': 0}]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_groups = [\n",
    "    piece.get_metadata(\n",
    "        lambda fn: pq.ParquetFile(open(fn, mode=\"rb\"))\n",
    "    ).row_group(0)\n",
    "    for piece in pieces\n",
    "]\n",
    "stats = []\n",
    "for row_group in row_groups:\n",
    "    s = {\"num-rows\": row_group.num_rows, \"columns\": []}\n",
    "    for i, name in enumerate(columns):\n",
    "        column = row_group.column(i)\n",
    "        d = {\"name\": name}\n",
    "        if column.statistics:\n",
    "            d.update(\n",
    "                {\n",
    "                    \"min\": column.statistics.min,\n",
    "                    \"max\": column.statistics.max,\n",
    "                    \"null_count\": column.statistics.null_count,\n",
    "                }\n",
    "            )\n",
    "        s[\"columns\"].append(d)\n",
    "    stats.append(s)\n",
    "stats[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `pieces` member of `ParquetDataset` class, it is straightforward to read back each partition of the dataset in parallel. Since we know the names of the columns and the *pieces* to iterate through, we can use the same `multiprocessing.Pool` approach that we used to write the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_partition(piece, columns):\n",
    "    with open(piece.path, mode=\"rb\") as f:\n",
    "        table = piece.read(\n",
    "            columns=columns,\n",
    "            use_pandas_metadata=True,\n",
    "            file=f,\n",
    "            use_threads=False\n",
    "        )\n",
    "    df = table.to_pandas(use_threads=False)\n",
    "    return df[list(columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24 ms, sys: 20 ms, total: 44 ms\n",
      "Wall time: 344 ms\n"
     ]
    }
   ],
   "source": [
    "def fread(args):\n",
    "    piece = args[0]\n",
    "    columns = args[1]\n",
    "    dfrd = read_partition(piece, columns=columns)\n",
    "    return dfrd\n",
    "\n",
    "fargs = [ [piece, columns] for piece in pieces ]\n",
    "p = Pool(nprocs)\n",
    "%time df_part_rd = p.map(fread, fargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can also confirm that each partition of `df_part_rd` is the same as `df_part`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(df_part_rd[i] == df_part[i] for i in range(npart))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning: This document should be viewed as my own personal notes.  There may be serious mistakes/errors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
